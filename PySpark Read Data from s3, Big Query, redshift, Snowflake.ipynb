{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163efda2",
   "metadata": {},
   "source": [
    "# Read Data from S3 Bucket\n",
    "\n",
    "Make sure to replace \"YOUR_ACCESS_KEY\", \"YOUR_SECRET_KEY\", \"your-bucket-name\", and \"path/to/your/file.csv\" with your own values. Also, note that you may need to adjust the file format and any additional options according to your specific file format and requirements.\n",
    "\n",
    "This code assumes you have the necessary AWS access key and secret access key to access your S3 bucket. If you don't have them set up, you can either pass them directly in the spark.conf.set() calls or use AWS CLI or environment variables to provide the necessary credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8886236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from S3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set AWS access key and secret access key\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", \"YOUR_ACCESS_KEY\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"YOUR_SECRET_KEY\")\n",
    "\n",
    "# Read data from S3 into a DataFrame\n",
    "bucket_name = \"your-bucket-name\"\n",
    "file_path = \"s3a://{}/path/to/your/file.csv\".format(bucket_name)\n",
    "\n",
    "my_data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "my_data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646a245",
   "metadata": {},
   "source": [
    "### Save to S3\n",
    "Make sure to replace \"YOUR_ACCESS_KEY\", \"YOUR_SECRET_KEY\", \"your-bucket-name\", and \"path/to/output\" with your own values. Additionally, adjust the file format and the save mode (mode()) as per your requirements.\n",
    "\n",
    "This code assumes you have the necessary AWS access key and secret access key to access your S3 bucket. If you don't have them set up, you can either pass them directly in the spark.conf.set() calls or use AWS CLI or environment variables to provide the necessary credentials.\n",
    "\n",
    "The mode(\"overwrite\") option is used to overwrite the data in the output path if it already exists. You can change it to \"append\" or \"ignore\" depending on your desired behavior when the output path already contains data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc67d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Save to S3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set AWS access key and secret access key\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", \"YOUR_ACCESS_KEY\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"YOUR_SECRET_KEY\")\n",
    "\n",
    "# Save DataFrame to S3\n",
    "bucket_name = \"your-bucket-name\"\n",
    "output_path = \"s3a://{}/path/to/output\".format(bucket_name)\n",
    "\n",
    "my_data.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7977120",
   "metadata": {},
   "source": [
    "# Read Data from Big Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee953dd",
   "metadata": {},
   "source": [
    "Make sure to replace \"YOUR_PROJECT_ID\", \"your-project-id\", \"your-dataset-id\", and \"your-table-name\" with your own values.\n",
    "\n",
    "This code assumes that you have already set up the necessary credentials and configurations to access your BigQuery data. You can refer to the official PySpark documentation for more details on configuring the connection to BigQuery and setting up the necessary authentication credentials.\n",
    "\n",
    "Please note that the spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation configuration is set to \"true\" to allow reading data from BigQuery into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from BigQuery\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Google Cloud Platform project ID\n",
    "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.project.id\", \"YOUR_PROJECT_ID\")\n",
    "\n",
    "# Read data from BigQuery into a DataFrame\n",
    "project_id = \"your-project-id\"\n",
    "dataset_id = \"your-dataset-id\"\n",
    "table_name = \"your-table-name\"\n",
    "\n",
    "table = \"[{}:{}.{}]\".format(project_id, dataset_id, table_name)\n",
    "\n",
    "my_data = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", table) \\\n",
    "    .load()\n",
    "\n",
    "# Display the DataFrame\n",
    "my_data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081c3cd0",
   "metadata": {},
   "source": [
    "### Save to Big Query\n",
    "\n",
    "Make sure to replace \"YOUR_PROJECT_ID\", \"your-project-id\", \"your-dataset-id\", and \"your-table-name\" with your own values.\n",
    "\n",
    "This code assumes that you have already set up the necessary credentials and configurations to access your BigQuery data. You can refer to the official PySpark documentation for more details on configuring the connection to BigQuery and setting up the necessary authentication credentials.\n",
    "\n",
    "Please note that the spark.sql.legacy.createNativeDataSourceTableWithLocation configuration is set to \"true\" to allow writing data from DataFrame to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3939e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Save to BigQuery\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Google Cloud Platform project ID\n",
    "spark.conf.set(\"spark.sql.legacy.createNativeDataSourceTableWithLocation\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.project.id\", \"YOUR_PROJECT_ID\")\n",
    "\n",
    "# Save DataFrame to BigQuery\n",
    "project_id = \"your-project-id\"\n",
    "dataset_id = \"your-dataset-id\"\n",
    "table_name = \"your-table-name\"\n",
    "\n",
    "table = \"[{}:{}.{}]\".format(project_id, dataset_id, table_name)\n",
    "\n",
    "my_data.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", table) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75cea4",
   "metadata": {},
   "source": [
    "# Read from Redshift\n",
    "Make sure to replace \"your-redshift-host\", \"your-database\", \"your-username\", \"your-password\", \"your-schema\", and \"your-table\" with your own values.\n",
    "\n",
    "This code assumes that you have already set up the necessary credentials and configurations to access your Redshift data. You'll need to provide the correct Redshift JDBC URL, username, and password. Additionally, you'll need to specify the Redshift JDBC driver class.\n",
    "\n",
    "Please note that you need to have the spark-redshift connector added to your PySpark environment. You can include it by providing the necessary Maven coordinates or by including the JAR file in your Spark configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad69ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from Redshift\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Redshift configurations\n",
    "spark.conf.set(\"spark.redshift.jdbc.url\", \"jdbc:redshift://your-redshift-host:5439/your-database\")\n",
    "spark.conf.set(\"spark.redshift.jdbc.username\", \"your-username\")\n",
    "spark.conf.set(\"spark.redshift.jdbc.password\", \"your-password\")\n",
    "spark.conf.set(\"spark.redshift.jdbc.driver\", \"com.amazon.redshift.jdbc.Driver\")\n",
    "\n",
    "# Read data from Redshift into a DataFrame\n",
    "my_data = spark.read \\\n",
    "    .format(\"com.databricks.spark.redshift\") \\\n",
    "    .option(\"url\", \"jdbc:redshift://your-redshift-host:5439/your-database\") \\\n",
    "    .option(\"dbtable\", \"your-schema.your-table\") \\\n",
    "    .load()\n",
    "\n",
    "# Display the DataFrame\n",
    "my_data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94840de",
   "metadata": {},
   "source": [
    "### Save to RedShift\n",
    "Make sure to replace \"your-redshift-host\", \"your-database\", \"your-username\", \"your-password\", \"your-schema\", and \"your-table\" with your own values.\n",
    "\n",
    "This code assumes that you have already set up the necessary credentials and configurations to access your Redshift data. You'll need to provide the correct Redshift JDBC URL, username, and password. Additionally, you'll need to specify the Redshift JDBC driver class.\n",
    "\n",
    "Please note that you need to have the spark-redshift connector added to your PySpark environment. You can include it by providing the necessary Maven coordinates or by including the JAR file in your Spark configuration.\n",
    "\n",
    "When saving to Redshift, you need to specify the Redshift table using the \"dbtable\" option, which should be in the format \"your-schema.your-table\". The \"mode(\"overwrite\")\" option is used to overwrite the data in the table if it already exists. You can change it to \"append\" or \"ignore\" depending on your desired behavior when the table already contains data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Save to Redshift\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Redshift configurations\n",
    "spark.conf.set(\"spark.redshift.jdbc.url\", \"jdbc:redshift://your-redshift-host:5439/your-database\")\n",
    "spark.conf.set(\"spark.redshift.jdbc.username\", \"your-username\")\n",
    "spark.conf.set(\"spark.redshift.jdbc.password\", \"your-password\")\n",
    "spark.conf.set(\"spark.redshift.jdbc.driver\", \"com.amazon.redshift.jdbc.Driver\")\n",
    "\n",
    "# Save DataFrame to Redshift\n",
    "my_data.write \\\n",
    "    .format(\"com.databricks.spark.redshift\") \\\n",
    "    .option(\"url\", \"jdbc:redshift://your-redshift-host:5439/your-database\") \\\n",
    "    .option(\"dbtable\", \"your-schema.your-table\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e5dbce",
   "metadata": {},
   "source": [
    "# Read From Snowflake\n",
    "Make sure to replace \"your-snowflake-url\", \"your-username\", \"your-password\", \"your-database\", \"your-warehouse\", \"your-schema\", \"your-role\", and \"your-table\" with your own values.\n",
    "\n",
    "This code assumes that you have already set up the necessary credentials and configurations to access your Snowflake data. You'll need to provide the correct Snowflake URL, username, password, database, warehouse, schema, and role.\n",
    "\n",
    "Please note that you need to have the spark-snowflake connector added to your PySpark environment. You can include it by providing the necessary Maven coordinates or by including the JAR file in your Spark configuration.\n",
    "\n",
    "When reading from Snowflake, you need to specify the Snowflake table using the \"dbtable\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bce339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from Snowflake\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Snowflake configurations\n",
    "spark.conf.set(\"spark.snowflake.url\", \"your-snowflake-url\")\n",
    "spark.conf.set(\"spark.snowflake.user\", \"your-username\")\n",
    "spark.conf.set(\"spark.snowflake.password\", \"your-password\")\n",
    "spark.conf.set(\"spark.snowflake.driver\", \"net.snowflake.spark.snowflake\")\n",
    "\n",
    "# Read data from Snowflake into a DataFrame\n",
    "my_data = spark.read \\\n",
    "    .format(\"net.snowflake.spark.snowflake\") \\\n",
    "    .option(\"sfURL\", \"your-snowflake-url\") \\\n",
    "    .option(\"sfDatabase\", \"your-database\") \\\n",
    "    .option(\"sfWarehouse\", \"your-warehouse\") \\\n",
    "    .option(\"sfSchema\", \"your-schema\") \\\n",
    "    .option(\"sfWarehouse\", \"your-warehouse\") \\\n",
    "    .option(\"sfRole\", \"your-role\") \\\n",
    "    .option(\"dbtable\", \"your-table\") \\\n",
    "    .load()\n",
    "\n",
    "# Display the DataFrame\n",
    "my_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e842ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293951a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
